=====Tokenization and Lemmatize ==================First
we
need
to
understand
the
basic
of
regression
and
what
parameter
of
the
equation
are
changed
when
using
a
specific
model
.
Simple
linear
regression
,
also
known
a
ordinary
least
square
(
OLS
)
attempt
to
minimize
the
sum
of
error
squared
.
The
error
in
this
case
is
the
difference
between
the
actual
data
point
and
it
predicted
value
.
The
equation
for
this
model
is
referred
to
a
the
cost
function
and
is
a
way
to
find
the
optimal
error
by
minimizing
and
measuring
it
.
The
gradient
descent
algorithm
is
used
to
find
the
optimal
cost
function
by
going
over
a
number
of
iteration
.
But
the
data
we
need
to
define
and
analyze
is
not
always
so
easy
to
characterize
with
the
base
OLS
model
.
One
situation
is
the
data
showing
multi-collinearity
,
this
is
when
predictor
variable
are
correlated
to
each
other
and
to
the
response
variable
.
To
picture
this
let
’
s
say
we
’
re
doing
a
study
that
look
at
a
response
variable
?
—
?
patient
weight
,
and
our
predictor
variable
would
be
height
,
sex
,
and
diet
.
The
problem
here
is
that
height
and
sex
are
also
correlated
and
can
inflate
the
standard
error
of
their
coefficient
which
may
make
them
seem
statistically
insignificant
.
Ridge
regression
us
L2
regularization
which
add
the
following
penalty
term
to
the
OLS
equation
.
The
L2
term
is
equal
to
the
square
of
the
magnitude
of
the
coefficient
.
In
this
case
if
lambda
(
?
)
is
zero
then
the
equation
is
the
basic
OLS
but
if
it
is
greater
than
zero
then
we
add
a
constraint
to
the
coefficient
.
This
constraint
result
in
minimized
coefficient
(
aka
shrinkage
)
that
trend
towards
zero
the
larger
the
value
of
lambda
.
Shrinking
the
coefficient
lead
to
a
lower
variance
and
in
turn
a
lower
error
value
.
Therefore
Ridge
regression
decrease
the
complexity
of
a
model
but
doe
not
reduce
the
number
of
variable
,
it
rather
just
shrink
their
effect
.
Lasso
regression
us
the
L1
penalty
term
and
stand
for
Least
Absolute
Shrinkage
and
Selection
Operator
.
The
penalty
applied
for
L2
is
equal
to
the
absolute
value
of
the
magnitude
of
the
coefficient
:
Similar
to
ridge
regression
,
a
lambda
value
of
zero
spit
out
the
basic
OLS
equation
,
however
given
a
suitable
lambda
value
lasso
regression
can
drive
some
coefficient
to
zero
.
The
larger
the
value
of
lambda
the
more
feature
are
shrunk
to
zero
.
This
can
eliminate
some
feature
entirely
and
give
u
a
subset
of
predictor
that
help
mitigate
multi-collinearity
and
model
complexity
.
Predictors
not
shrunk
towards
zero
signify
that
they
are
important
and
thus
L1
regularization
allows
for
feature
selection
(
sparse
selection
)
.
In
addition
to
setting
and
choosing
a
lambda
value
elastic
net
also
allows
u
to
tune
the
alpha
parameter
where
?
?
=
0
corresponds
to
ridge
and
?
?
=
1
to
lasso
.
Simply
put
,
if
you
plug
in
0
for
alpha
,
the
penalty
function
reduces
to
the
L1
(
ridge
)
term
and
if
we
set
alpha
to
1
we
get
the
L2
(
lasso
)
term
.
Therefore
we
can
choose
an
alpha
value
between
0
and
1
to
optimize
the
elastic
net
.
Effectively
this
will
shrink
some
coefficient
and
set
some
to
0
for
sparse
selection
.
Here
we
perform
a
cross
validation
and
take
a
peek
at
the
lambda
value
corresponding
to
the
lowest
prediction
error
before
fitting
the
data
to
the
model
and
viewing
the
coefficient
.
Performing
Lasso
regression
Performing
Elastic
Net
regression
We
can
see
that
the
R
mean-squared
value
using
all
three
model
were
very
close
to
each
other
,
but
both
did
marginally
perform
better
than
ridge
regression
(
Lasso
having
done
best
)
.
Lasso
regression
also
showed
the
highest
R²
value
.
